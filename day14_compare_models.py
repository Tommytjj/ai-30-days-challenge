# compare_models.py â€”â€” Day 14 æ¨¡å‹å¯¹æ¯”ï¼ˆæ”¯æŒç¦»çº¿ fallbackï¼‰
import joblib
import json
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import os

# ==============================
# 1. åŠ è½½æ•°æ®ï¼ˆå¸¦ fallbackï¼Œä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼ï¼‰
# ==============================
print("ğŸ“¥ æ­£åœ¨åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®ï¼ˆç”¨äºè¯„ä¼°ï¼‰...")

try:
    from sklearn.datasets import fetch_california_housing
    housing = fetch_california_housing()
    print("âœ… ä½¿ç”¨çœŸå®åŠ å·æˆ¿ä»·æ•°æ®")
except Exception as e:
    print(f"âš ï¸ çœŸå®æ•°æ®åŠ è½½å¤±è´¥ ({type(e).__name__})ï¼Œåˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ•°æ®...")
    from sklearn.datasets import make_regression
    
    X_sim, y_sim = make_regression(
        n_samples=20640,
        n_features=8,
        noise=100,
        random_state=42
    )
    # ç¼©æ”¾åˆ°åˆç†æˆ¿ä»·èŒƒå›´ [0.15, 5.0]
    y_sim = (y_sim - y_sim.min()) / (y_sim.max() - y_sim.min())
    y_sim = y_sim * (5.0 - 0.15) + 0.15
    
    class MockHousing:
        def __init__(self):
            self.data = X_sim
            self.target = y_sim
            self.feature_names = [
                'MedInc', 'HouseAge', 'AveRooms', 'AveBedrms',
                'Population', 'AveOccup', 'Latitude', 'Longitude'
            ]
    housing = MockHousing()
    print("âœ… ä½¿ç”¨æ¨¡æ‹ŸåŠ å·æˆ¿ä»·æ•°æ®ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰")

X, y = housing.data, housing.target

# åˆ’åˆ†æµ‹è¯•é›†ï¼ˆå¿…é¡»å’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼ï¼‰
_, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==============================
# 2~5. åŸæœ‰è¯„ä¼°é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
# ==============================
os.makedirs('evals', exist_ok=True)

model_configs = [
    {
        'name': 'Linear Regression',
        'path': 'E:/AI_learning/models/california_housing_pipeline_v1_linear.joblib',
        'type': 'regressor'
    },
    {
        'name': 'Random Forest',
        'path': 'E:/AI_learning/models/california_housing_pipeline_v1_rf.joblib',
        'type': 'regressor'
    }
]


results = []
for config in model_configs:
    try:
        print(f"ğŸ” è¯„ä¼°æ¨¡å‹: {config['name']}")
        if not os.path.exists(config['path']):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {config['path']}")
        
        model = joblib.load(config['path'])
        y_pred = model.predict(X_test)
        
        mae = float(mean_absolute_error(y_test, y_pred))
        rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))
        r2 = float(r2_score(y_test, y_pred))
        negative_count = int((y_pred < 0).sum())
        
        result = {
            'model_name': config['name'],
            'mae': round(mae, 4),
            'rmse': round(rmse, 4),
            'r2': round(r2, 4),
            'negative_predictions': negative_count,
            'is_business_safe': negative_count == 0
        }
        results.append(result)
        print(f"  âœ… MAE: ${mae:.2f}k, RÂ²: {r2:.4f}, è´Ÿé¢„æµ‹: {negative_count}")
        
    except Exception as e:
        print(f"  âŒ è¯„ä¼°å¤±è´¥: {e}")
        results.append({
            'model_name': config['name'],
            'error': str(e)
        })

# ä¿å­˜æŠ¥å‘Š
report_path = 'E:/AI_learning/evals/model_comparison_day14.json'
with open(report_path, 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"\nğŸ“Š æ¨¡å‹å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

# æ¨èé€»è¾‘
safe_models = [r for r in results if r.get('is_business_safe', False)]
if safe_models:
    best = max(safe_models, key=lambda x: x['r2'])
    print(f"\nğŸ† æ¨èæ¨¡å‹: {best['model_name']} (RÂ²={best['r2']:.4f}, æ— è´Ÿé¢„æµ‹)")
else:
    print("\nâš ï¸ è­¦å‘Šï¼šæ‰€æœ‰æ¨¡å‹å‡å­˜åœ¨è´Ÿé¢„æµ‹ï¼")

    # 6. ç”Ÿæˆ Markdown æŠ¥å‘Šï¼ˆç”¨äº README æˆ–æ–‡æ¡£ï¼‰
md_lines = ["# ğŸ“Š Day 14 æ¨¡å‹ A/B æµ‹è¯•æŠ¥å‘Š\n"]
md_lines.append("| æ¨¡å‹ | MAE ($k) | RMSE ($k) | RÂ² | è´Ÿé¢„æµ‹æ•° | å®‰å…¨ |")
md_lines.append("|------|----------|-----------|-----|----------|------|")

for r in results:
    if 'error' not in r:
        safe_icon = "âœ…" if r['is_business_safe'] else "âŒ"
        md_lines.append(
            f"| {r['model_name']} | {r['mae']:.2f} | {r['rmse']:.2f} | {r['r2']:.4f} | {r['negative_predictions']} | {safe_icon} |"
        )


with open('E:/AI_learning/evals/model_comparison_day14.md', 'w', encoding='utf-8') as f:
    f.write('\n'.join(md_lines))


print("ğŸ“„ Markdown æŠ¥å‘Šå·²ç”Ÿæˆ: E:/AI_learning/evals/model_comparison_day14.md")