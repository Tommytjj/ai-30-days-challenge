# tune_rf_regression.py â€”â€” Day 15 è¶…å‚æ•°è°ƒä¼˜ï¼ˆæ”¯æŒç¦»çº¿ & ä¸ Day13/14 ä¸€è‡´ï¼‰
import os
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import joblib
import numpy as np

# ==============================
# 1. åŠ è½½æ•°æ®ï¼ˆä¸ Day 13/14 å®Œå…¨ä¸€è‡´çš„ fallback é€»è¾‘ï¼ï¼‰
# ==============================
print("ğŸ“¥ åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®...")

try:
    from sklearn.datasets import fetch_california_housing
    housing = fetch_california_housing()
    X, y = housing.data, housing.target
    print("âœ… ä½¿ç”¨çœŸå®åŠ å·æˆ¿ä»·æ•°æ®")
except Exception as e:
    print(f"âš ï¸ çœŸå®æ•°æ®åŠ è½½å¤±è´¥ ({type(e).__name__})ï¼Œåˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ•°æ®...")
    from sklearn.datasets import make_regression
    
    # âš ï¸ å…³é”®ï¼šå¿…é¡»å’Œ Day13/14 çš„æ¨¡æ‹Ÿé€»è¾‘å®Œå…¨ä¸€è‡´ï¼
    X, y = make_regression(
        n_samples=20640,
        n_features=8,
        n_informative=6,
        noise=100,
        random_state=42
    )
    # ç¼©æ”¾åˆ°çœŸå®æˆ¿ä»·èŒƒå›´ [0.15, 5.0]ï¼ˆå•ä½ï¼šåƒç¾å…ƒï¼‰
    y = (y - y.min()) / (y.max() - y.min())  # å½’ä¸€åŒ–åˆ° [0, 1]
    y = y * (5.0 - 0.15) + 0.15              # ç¼©æ”¾åˆ° [0.15, 5.0]
    print("âœ… ä½¿ç”¨æ¨¡æ‹ŸåŠ å·æˆ¿ä»·æ•°æ®ï¼ˆç¦»çº¿æ¨¡å¼ï¼Œä¸ Day13/14 ä¸€è‡´ï¼‰")

print(f"ğŸ“Š æ•°æ®è§„æ¨¡: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")

# ==============================
# 2. åˆ’åˆ†æ•°æ®é›†ï¼ˆrandom_state=42 ä¿è¯ä¸€è‡´æ€§ï¼‰
# ==============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ==============================
# 3. è¶…å‚æ•°è°ƒä¼˜
# ==============================
param_dist = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [None, 10, 20, 30, 40],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],  # æ–°å¢ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
    # æ³¨æ„ï¼šrandom_state ä¸åœ¨æ­¤å¤„æœç´¢ï¼Œç›´æ¥åœ¨æ¨¡å‹ä¸­å›ºå®š
}

# åˆ›å»ºåŸºç¡€æ¨¡å‹ï¼ˆå›ºå®š random_state ä¿è¯å¯å¤ç°ï¼‰
rf = RandomForestRegressor(random_state=42)

print("ğŸ” å¼€å§‹è¶…å‚æ•°éšæœºæœç´¢ï¼ˆ5 æŠ˜äº¤å‰éªŒè¯ï¼Œ20 ç»„ç»„åˆï¼‰...")
search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dist,
    n_iter=20,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=1,
    random_state=42  # æ§åˆ¶æœç´¢çš„éšæœºæ€§
)

search.fit(X_train, y_train)

# ==============================
# 4. è¾“å‡ºç»“æœ & ä¿å­˜æ¨¡å‹
# ==============================
print("\nâœ… æœ€ä½³è¶…å‚æ•°:")
best_params = search.best_params_
for k, v in best_params.items():
    print(f"  {k}: {v}")

print(f"ğŸ† äº¤å‰éªŒè¯æœ€ä½³ RÂ²: {search.best_score_:.4f}")

# è¯„ä¼°æµ‹è¯•é›†
best_model = search.best_estimator_
y_pred = best_model.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
test_r2 = r2_score(y_test, y_pred)

print(f"\nğŸ§ª æµ‹è¯•é›†æ€§èƒ½:")
print(f"  MAE: ${test_mae:.2f}k")
print(f"  RÂ²: {test_r2:.4f}")

# ä¿å­˜æ¨¡å‹
os.makedirs('models', exist_ok=True)
model_path = 'E:/AI_learning/models/regressor_v2_rf_tuned.joblib'
joblib.dump(best_model, model_path)
print(f"\nğŸ’¾ è°ƒä¼˜åæ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

# åœ¨ tune_rf_regression.py æœ«å°¾æ·»åŠ 
#  åœ¨ evals/ ç›®å½•ä¸‹ä¿å­˜è°ƒä¼˜æ—¥å¿—
import json
tuning_log = {
    'best_params': best_params,
    'cv_best_score': float(search.best_score_),
    'test_mae': float(test_mae),
    'test_r2': float(test_r2),
    'data_source': 'simulated (offline fallback)'
}
with open('E:/AI_learning/evals/tuning_log_day15.json', 'w') as f:
    json.dump(tuning_log, f, indent=2)